<h2>Asteroids: Analysis</h2>
<h3>A non-distributed solution to the problem</h3>
<p>
The quickest way to get going with this problem is to model it as a <a href="https://en.wikipedia.org/wiki/Longest_path_problem">longest path</a> problem in a graph. As the linked article mentions, this is NP-complete in general, but can be solved in linear time in particular cases like the one we have at hand. If we consider the edges the possible moves after a turn is resolved, the destination row is always above the source row for any edge and thus the graph is acyclic. Moreover, we have exactly 3 incoming edges to each position (<i>i</i>, <i>j</i>): you can arrive by either staying in the same column from(<i>i-1</i>, <i>j</i>), moving right from (<i>i</i>, <i>j-1</i>) or moving left from (<i>i</i>, <i>j+1</i>). In each case you need to check that all involved positions are valid (i.e., not outside the screen) and not asteroids. The cost of the edge is, of course, the sum of the involved cells (only include one of the source cell and destination cell, as the destination of one edge is the source of the next). In this way, we have a graph with at most 3 &times; GetHeight() &times; GetWidth() edges and we can calculate the longest path on it in linear time.
</p><p>
One thing to notice, as it is usual for this kind of dynamic programming approach to paths in graphs, is that we don't have to construct the graph explicitly. We just calculate the result from each position as the best possible from each possible previous position. Calculating all positions in row i only requires the result from positions in row i-1, so we can use O(GetWidth()) memory in total (instead of a linear size memory). This makes a single node solution comply with memory limits even for the Large dataset. However, as you might expect, it would be too slow.
</p>
<h3>How to distribute the solution</h3>
<p>
Our single node solution uses the results in row <i>i</i> to calculate results in row <i>i</i> + 1. Unfortunately, this is a deep dependency; we can't assign a subset of rows to each node because only one new row can be calculated at any given time, making the solution be effectively non-distributed, since only one node is doing computation at any given time.
</p><p>
An additional observation is that, since the dependency for a given cell is three contiguous cells, and they overlap with the dependencies of neighboring cells, a slice of columns [<i>j</i>, <i>k</i>) in row <i>i + 1</i> has a dependency of only a slightly bigger slice of columns  [<i>j</i> - 1, <i>k</i> + 1) in row <i>i</i>. This yields a first idea: we can make each node calculate a slice of row i, and then synchronize with its neighbors to send the additional cell that each needs for their next rows. However, in the Large dataset, this implies 30,000 steps of message exchanging, which is too much overhead. With a baseline latency of 5ms, 30,000 message exchanges would result in 150 seconds of waiting messages alone!
</p><p>
We can do better if we delay the exchange of messages: If want to calculate <i>r</i> rows ahead, we need <i>r</i> extra cells on each side of our slice for the dependency (notice that each row adds one cell on each side). Therefore, we can calculate about GetWidth() / NumberOfNodes() steps (watch out the rounding) locally, and then exchange GetWidth() / NumberOfNodes() extra cells with each neighbor. This maintains more or less the same amount of total size of messages, but reduces the number of messages by a factor of NumberOfNodes(), which is significant because network latency, which is more important than network speed in cases with many messages, is multiplied by the number of synchronizing steps.
This trick is finally enough to pass the Large dataset.
</p>
